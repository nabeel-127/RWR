2023.02.22

K-Means
	Input Parameters
		k = number of clusters
	Algorithm
		(https://www.javatpoint.com/k-means-clustering-algorithm-in-machine-learning)
		1. Select the number K to decide the number of clusters.
		2. Select random K points or centroids. (It can be other from the input dataset).
		3. Assign each data point to their closest centroid, which will form the predefined K clusters.
		4. Calculate the variance and place a new centroid of each cluster.
		5. Repeat the third steps, which means reassign each datapoint to the new closest centroid of each cluster.
		6. If any reassignment occurs, then go to step-4 else go to FINISH.
		7. The model is ready.

K-Nearest Neighbor (K-NN)
	Input Parameters
		k = number of neighbors to consider
	Algorithm
		(https://www.javatpoint.com/k-nearest-neighbor-algorithm-for-machine-learning)
		1. Select the number K of the neighbors
		2. Calculate the Euclidean distance of K number of neighbors
		3. Take the K nearest neighbors as per the calculated Euclidean distance.
		4. Among these k neighbors, count the number of the data points in each category.
		5. Assign the new data points to that category for which the number of the neighbor is maximum.
		6. Our model is ready.

DBSCAN
	Input Parameters
		epsilon = neighborhood around a data point i.e. if the distance between two points is lower or equal to epsilon then they are considered neighbors
		minPts = minimum number of data points to define a cluster
	Algorithm
		(https://www.geeksforgeeks.org/dbscan-clustering-in-ml-density-based-clustering/)
		1. Find all the neighbor points within eps and identify the core points or visited with more than MinPts neighbors.
		2. For each core point if it is not already assigned to a cluster, create a new cluster.
		3. Find recursively all its density connected points and assign them to the same cluster as the core point. A point a and b are said to be density connected if there exist a point c which has a sufficient number of points in its neighbors and both the points a and b are within the eps distance. This is a chaining process. So, if b is neighbor of c, c is neighbor of d, d is neighbor of e, which in turn is neighbor of a implies that b is neighbor of a.
		4. Iterate through the remaining unvisited points in the dataset. Those points that do not belong to any cluster are noise.

